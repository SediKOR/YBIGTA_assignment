{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SediKOR/Ybigta_assignment/blob/main/KoBERT_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBERT를 이용한 한국어 욕설 데이터 감지 모델 제작\n",
        "\n",
        "- 다음 코드(감성분석)를 참고하였습니다.\n",
        "- 네이버 감성분석 90% 달성하기.ipynb\n",
        "\n",
        "  - https://github.com/kimwoonggon/publicservant_AI/blob/master/2_(Hugging%2B%ED%95%9C%EA%B8%80BERT)%EB%84%A4%EC%9D%B4%EB%B2%84%20%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D%2090%25%20%EB%8B%AC%EC%84%B1%ED%95%98%EA%B8%B0.ipynb"
      ],
      "metadata": {
        "id": "cWKl8ny2UnMw"
      },
      "id": "cWKl8ny2UnMw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "실행 위한 library 설치"
      ],
      "metadata": {
        "id": "IapLjAgcsRdO"
      },
      "id": "IapLjAgcsRdO"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a54334d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a54334d4",
        "outputId": "817ec813-b07e-436b-ba9a-ca1e37ba474d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee5bedc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee5bedc6",
        "outputId": "64532144-61bb-4a42-c8f2-626f8a6e9ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# colab에서 transformer 실행시 발생하는 오류 해결하는 코드\n",
        "!pip install jax==0.4.13\n",
        "!pip install jaxlib==0.4.13"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXiH6qf7FTUq",
        "outputId": "845d4f99-2f9c-4a4b-f499-b8d241a01c7d"
      },
      "id": "HXiH6qf7FTUq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.13\n",
            "  Downloading jax-0.4.13.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.13) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.13) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.13) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.13) (1.10.1)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.13-py3-none-any.whl size=1518704 sha256=a75c8e998c46c84c64a593389f657ab393699a0ef26d9839d95bbb6d19fe418d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/25/f297f69029b5e4064e4736a0c4b3996a44cc27781c120bcb99\n",
            "Successfully built jax\n",
            "Installing collected packages: jax\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.25\n",
            "    Uninstalling jax-0.3.25:\n",
            "      Successfully uninstalled jax-0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f720a9d0",
      "metadata": {
        "id": "f720a9d0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import random as rd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532ebb50",
      "metadata": {
        "id": "532ebb50"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForMaskedLM, TFAutoModelWithLMHead\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# random seed 설정\n",
        "random_seed = 13\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "torch_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "tf_model = TFAutoModelWithLMHead.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be7cafd",
      "metadata": {
        "id": "6be7cafd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 load\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/concatenated_dataset_train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/concatenated_dataset_train.csv\")"
      ],
      "metadata": {
        "id": "yuxAeStONH6t"
      },
      "id": "yuxAeStONH6t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c42bc68",
      "metadata": {
        "id": "0c42bc68"
      },
      "outputs": [],
      "source": [
        "# train shape 확인\n",
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test shape 확인\n",
        "test.shape"
      ],
      "metadata": {
        "id": "KXfFl1fPNqQs"
      },
      "id": "KXfFl1fPNqQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69cdecf6",
      "metadata": {
        "id": "69cdecf6"
      },
      "outputs": [],
      "source": [
        "# dataframe 확인\n",
        "train[50:70]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774627fe",
      "metadata": {
        "id": "774627fe"
      },
      "source": [
        "# 버트 인풋 만들기\n",
        "한글 데이터를 분석하려면, 100개가 넘는 언어에 대해 훈련된 버트를 사용해야 합니다.\n",
        "이번에는 한국어 데이터로 훈련되었고, SKT에서 만든 KoBERT를 사용하도록 하겠습니다.\n",
        "모델을 로드하기에 앞서, 토크나이저를 불러오도록 하겠습니다.\n",
        "huggingface에서는 아주 쉽게 토크나이저를 불러올 수 있습니다.\n",
        "https://github.com/monologg/KoBERT-NER 에서 kobert를 tokenize 할 수 있는 코드를 가져왔습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe7a927",
      "metadata": {
        "id": "bfe7a927"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece `_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.token2idx, **self.added_tokens_encoder)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A KoBERT sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A KoBERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbb71aa",
      "metadata": {
        "id": "8bbb71aa"
      },
      "outputs": [],
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9bb3dd",
      "metadata": {
        "id": "ed9bb3dd"
      },
      "source": [
        "버트를 사용하기에 앞서 가장 기초에 속하는 tokenizer 사용 방법에 대해서 잠시 배워보도록 하겠습니다.\n",
        "tokenizer.encode => 문장을 버트 모델의 인풋 토큰값으로 바꿔줌\n",
        "tokenizer.tokenize => 문장을 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "275b6d79",
      "metadata": {
        "id": "275b6d79"
      },
      "outputs": [],
      "source": [
        "# 인코딩 예시\n",
        "print(tokenizer.encode(\"이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. 근데 이런 개막장 드라마는 도대체 누가 보느냐면 변태들이 보는 것이다. 정상적인 사람들은 채널을 돌리게 된다.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc10488a",
      "metadata": {
        "id": "fc10488a"
      },
      "outputs": [],
      "source": [
        "# 토크나이저 예시\n",
        "print(tokenizer.tokenize(\"이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. 근데 이런 개막장 드라마는 도대체 누가 보느냐면 변태들이 보는 것이다. 정상적인 사람들은 채널을 돌리게 된다.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60b75be",
      "metadata": {
        "id": "f60b75be"
      },
      "outputs": [],
      "source": [
        "# 패딩 예시\n",
        "print(tokenizer.encode(\"이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. 근데 이런 개막장 드라마는 도대체 누가 보느냐면 변태들이 보는 것이다. 정상적인 사람들은 채널을 돌리게 된다.\", max_length=64, pad_to_max_length=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0328f36c",
      "metadata": {
        "id": "0328f36c"
      },
      "outputs": [],
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "\n",
        "    SEQ_LEN = 128 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
        "\n",
        "    tokens, masks, segments, targets = [], [], [], []\n",
        "\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        # token : 문장을 토큰화함\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "\n",
        "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "\n",
        "        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
        "        tokens.append(token)\n",
        "        masks.append(mask)\n",
        "        segments.append(segment)\n",
        "\n",
        "        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "\n",
        "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return [tokens, masks, segments], targets\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 20\n",
        "# 긍부정 문장을 포함하고 있는 칼럼\n",
        "DATA_COLUMN = \"content\"\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
        "LABEL_COLUMN = \"label\"\n",
        "\n",
        "# train 데이터를 버트 인풋에 맞게 변환\n",
        "train_x, train_y = load_data(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80e94cf",
      "metadata": {
        "id": "c80e94cf"
      },
      "outputs": [],
      "source": [
        "# 훈련 성능을 검증한 test 데이터를 버트 인풋에 맞게 변환\n",
        "test_x, test_y = load_data(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb529f12",
      "metadata": {
        "id": "fb529f12"
      },
      "source": [
        "# BERT 활용한 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27b03fb",
      "metadata": {
        "id": "f27b03fb"
      },
      "outputs": [],
      "source": [
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
        "# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_outputs = bert_outputs[1]\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "sentiment_model.compile(optimizer=tf.keras.optimizers.Adam(lr=1.0e-5), loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "MNf7aTqkGXsX"
      },
      "id": "MNf7aTqkGXsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.summary()"
      ],
      "metadata": {
        "id": "bNezzt3aGqR_"
      },
      "id": "bNezzt3aGqR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rectified Adam 옵티마이저 사용\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=1.0e-5, weight_decay=0.0025, warmup_proportion=0.05)"
      ],
      "metadata": {
        "id": "SOH7gsUUGt6F"
      },
      "id": "SOH7gsUUGt6F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sentiment_bert():\n",
        "  # 버트 pretrained 모델 로드\n",
        "  model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "  # 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n",
        "  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "  segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "  # 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n",
        "  bert_outputs = model([token_inputs, mask_inputs, segment_inputs])\n",
        "\n",
        "  bert_outputs = bert_outputs[1]\n",
        "  sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(bert_outputs)\n",
        "  sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "\n",
        "  sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n",
        "  return sentiment_model"
      ],
      "metadata": {
        "id": "wSTtHHs3Gwio"
      },
      "id": "wSTtHHs3Gwio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TPU 실행 시\n",
        "# if TPU:\n",
        "#   strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "# # 함수를 strategy.scope로 묶어 줌\n",
        "#   with strategy.scope():\n",
        "#     sentiment_model = create_sentiment_bert()\n",
        "\n",
        "#   sentiment_model.fit(train_x, train_y, epochs=4, shuffle=True, batch_size=100, validation_data=(test_x, test_y))\n",
        "# else:\n",
        "  # GPU 모드로 훈련시킬 때\n",
        "sentiment_model = create_sentiment_bert()"
      ],
      "metadata": {
        "id": "9KfSenu1JL6X"
      },
      "id": "9KfSenu1JL6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model.fit(train_x, train_y, epochs=4, shuffle=True, batch_size=100, validation_data=(test_x, test_y))"
      ],
      "metadata": {
        "id": "RFbjBtLPJmmj"
      },
      "id": "RFbjBtLPJmmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d92be816",
      "metadata": {
        "id": "d92be816"
      },
      "outputs": [],
      "source": [
        "# 훈련 모델의 예측 성능을 F1 SCORE로 체크하기 위한 작업\n",
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def predict_load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904b45d6",
      "metadata": {
        "id": "904b45d6"
      },
      "outputs": [],
      "source": [
        "# test 데이터 예측하기\n",
        "test_set = predict_load_data(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0867f9a",
      "metadata": {
        "id": "f0867f9a"
      },
      "outputs": [],
      "source": [
        "test_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TPU를 사용하기 위해서\n",
        "with strategy.scope():\n",
        "  preds = sentiment_model.predict(test_set)"
      ],
      "metadata": {
        "id": "sO1LlexTHByE"
      },
      "id": "sO1LlexTHByE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f6a637",
      "metadata": {
        "id": "41f6a637"
      },
      "outputs": [],
      "source": [
        "# 부정이면 0, 긍정이면 1 출력\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ed7424",
      "metadata": {
        "id": "61ed7424"
      },
      "source": [
        "우리가 훈련한 모델을 F1 SCORE를 바탕으로 성능 측정\n",
        "F1 SCORE는 precision과 recall을 가중평균하여 계산합니다\n",
        "recall은 (모델이 TRUE라고 판정한 것의 숫자)/(전체 TRUE의 숫자)\n",
        "precision은 (진짜 TRUE) / (모델이 TRUE라고 판정한 것의 숫자)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3467e9",
      "metadata": {
        "id": "fd3467e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = test['label']\n",
        "# F1 Score 확인\n",
        "print(classification_report(y_true, np.round(preds,0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bf37cb",
      "metadata": {
        "id": "e6bf37cb"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f466372",
      "metadata": {
        "id": "2f466372"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 실제 데이터 적용하기\n",
        "입력된 텍스트를 바탕으로 욕설인지 아닌지 확률을 가지고 판단"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_convert_data(data):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    token = tokenizer.encode(data, max_length=SEQ_LEN, truncation=True, padding='max_length')\n",
        "\n",
        "    num_zeros = token.count(0)\n",
        "    mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "    segment = [0]*SEQ_LEN\n",
        "\n",
        "    tokens.append(token)\n",
        "    segments.append(segment)\n",
        "    masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "\n",
        "def movie_evaluation_predict(sentence):\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = sentiment_model.predict(data_x)\n",
        "    predict_value = np.ravel(predict)\n",
        "    predict_answer = np.round(predict_value,0).item()\n",
        "\n",
        "    if predict_answer == 0:\n",
        "      print(\"(부정 확률 : %.2f) 욕설 O \" % (1-predict_value))\n",
        "    elif predict_answer == 1:\n",
        "      print(\"(긍정 확률 : %.2f) 욕설 X\" % predict_value)"
      ],
      "metadata": {
        "id": "NxuY4w38HZlx"
      },
      "id": "NxuY4w38HZlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_evaluation_predict(\"개병신쉨ㅋㅋㅋㅋ\")"
      ],
      "metadata": {
        "id": "Pd1QX9JTHheA"
      },
      "id": "Pd1QX9JTHheA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c699589",
      "metadata": {
        "id": "4c699589"
      },
      "outputs": [],
      "source": [
        "def evaluate_text(text, threshold=0.90):\n",
        "    data_x = sentence_convert_data(text)\n",
        "    predict = classification_model.predict(data_x)\n",
        "    predict_value = predict[0][0]\n",
        "\n",
        "    if predict_value >= threshold:\n",
        "        print(\"(욕설 확률 : %.2f) 욕설 OOO\" % predict_value)\n",
        "    else:\n",
        "        print(\"(욕설 확률 : %.2f) 욕설 XXX\" % (1 - predict_value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59d5e81",
      "metadata": {
        "id": "f59d5e81"
      },
      "outputs": [],
      "source": [
        "evaluate_text(\"겜 그따구로 하지좀마\") # X\n",
        "evaluate_text(\"느금마\") # O\n",
        "evaluate_text('띠발') # O\n",
        "evaluate_text('씨발') # O\n",
        "evaluate_text('씨2발') # O\n",
        "evaluate_text('개같은새끼야') # O\n",
        "evaluate_text('전염병') # X\n",
        "evaluate_text('자지마') # X\n",
        "evaluate_text('보지마') # X\n",
        "evaluate_text('자1지마') # X\n",
        "evaluate_text('보1지마') # X\n",
        "evaluate_text('tlqkf') # O\n",
        "evaluate_text('Tlqkf') # O"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과\n",
        "1/1 [==============================] - 0s 51ms/step\n",
        "(욕설 확률 : 0.93) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 53ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 49ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 52ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 63ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 52ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 53ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설이 아닐 가능성이 높습니다.\n",
        "1/1 [==============================] - 0s 54ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 67ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 56ms/step\n",
        "(욕설 확률 : 0.99) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 60ms/step\n",
        "(욕설 확률 : 0.98) 입력 문장은 욕설일 가능성이 있습니다.\n",
        "1/1 [==============================] - 0s 53ms/step\n",
        "(욕설 확률 : 0.11) 입력 문장은 욕설이 아닐 가능성이 높습니다.\n",
        "1/1 [==============================] - 0s 81ms/step\n",
        "(욕설 확률 : 0.98) 입력 문장은 욕설일 가능성이 있습니다."
      ],
      "metadata": {
        "id": "19eyofm1Uagq"
      },
      "id": "19eyofm1Uagq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f106cc",
      "metadata": {
        "id": "25f106cc"
      },
      "outputs": [],
      "source": [
        "# 모델 저장\n",
        "# 모델 저장 변수 이름.save_pretrained(원하는 디렉토리) 형태\n",
        "torch_model.save_pretrained('model.pt') # 파이토치 기반 모델\n",
        "tf_model.save_pretrained('model.h5') # 텐서플로우 기반 모델"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델저장v2\n",
        "# torch_model = BertForMaskedLM.from_pretrained('model_0819.pt')\n",
        "# tf_model = TFAutoModelWithLMHead.from_pretrained('model_0819.h5')"
      ],
      "metadata": {
        "id": "Ejw6ZQxnUBZG"
      },
      "id": "Ejw6ZQxnUBZG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장한 모델 경로 이동 위함\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u6As8FagT2AA"
      },
      "id": "u6As8FagT2AA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장한 모델 google drive에 이동\n",
        "\n",
        "import shutil\n",
        "\n",
        "# 모델 파일 이동\n",
        "source_path = \"/content/model.h5\"  # 모델 파일의 경로와 이름을 지정해주세요\n",
        "destination_path = \"/content/drive/MyDrive/model.h5\"  # 이동할 목적지 경로와 이름을 지정해주세요\n",
        "\n",
        "# shutil.move()를 사용하여 파일 이동\n",
        "shutil.move(source_path, destination_path)"
      ],
      "metadata": {
        "id": "UNAa5nlfT6sY"
      },
      "id": "UNAa5nlfT6sY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}